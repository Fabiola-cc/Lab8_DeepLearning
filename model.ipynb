{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573c2e25",
   "metadata": {},
   "source": [
    "# Laboratorio 8\n",
    "Datos de [\"Store Item Demand Forecasting Challenge\"](https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d55446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b1f42",
   "metadata": {},
   "source": [
    "### Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3d6bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: (913000, 4)\n",
      "\n",
      "Primeras filas:\n",
      "        date  store  item  sales\n",
      "0 2013-01-01      1     1     13\n",
      "1 2013-01-02      1     1     11\n",
      "2 2013-01-03      1     1     14\n",
      "3 2013-01-04      1     1     13\n",
      "4 2013-01-05      1     1     10\n",
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype         \n",
      "---  ------  --------------   -----         \n",
      " 0   date    913000 non-null  datetime64[ns]\n",
      " 1   store   913000 non-null  int64         \n",
      " 2   item    913000 non-null  int64         \n",
      " 3   sales   913000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(3)\n",
      "memory usage: 27.9 MB\n",
      "None\n",
      "\n",
      "Valores faltantes\n",
      "date     0\n",
      "store    0\n",
      "item     0\n",
      "sales    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# usamos solo los datos en train para validar las respuestas luego\n",
    "df = pd.read_csv(\"data/train.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Revisión de estructura\n",
    "print(f\"Datos cargados: {df.shape}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Convertir fecha a datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Revisar valores faltantes\n",
    "print(\"\\nValores faltantes\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0086e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas duplicadas: 0\n",
      "\n",
      "Estadísticas de ventas:\n",
      "count    913000.000000\n",
      "mean         52.250287\n",
      "std          28.801144\n",
      "min           0.000000\n",
      "25%          30.000000\n",
      "50%          47.000000\n",
      "75%          70.000000\n",
      "max         231.000000\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Outliers detectados: 357 (0.04%)\n"
     ]
    }
   ],
   "source": [
    "# ya que no hay datos faltantes en el dataset no es necesario trabajarlo\n",
    "\n",
    "# Verificar duplicados\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {duplicados}\")\n",
    "if duplicados > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "# Estadísticas básicas\n",
    "print(f\"\\nEstadísticas de ventas:\")\n",
    "print(df['sales'].describe())\n",
    "    \n",
    "# Detectar outliers usando IQR\n",
    "Q1 = df['sales'].quantile(0.15)\n",
    "Q3 = df['sales'].quantile(0.85)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = df[(df['sales'] < lower_bound) | (df['sales'] > upper_bound)]\n",
    "print(f\"\\nOutliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Capping en lugar de eliminación\n",
    "df['sales'] = df['sales'].clip(lower=lower_bound, upper=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50363279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                date          store           item  \\\n",
      "count                         913000  913000.000000  913000.000000   \n",
      "mean   2015-07-02 11:59:59.999999744       5.500000      25.500000   \n",
      "min              2013-01-01 00:00:00       1.000000       1.000000   \n",
      "25%              2014-04-02 00:00:00       3.000000      13.000000   \n",
      "50%              2015-07-02 12:00:00       5.500000      25.500000   \n",
      "75%              2016-10-01 00:00:00       8.000000      38.000000   \n",
      "max              2017-12-31 00:00:00      10.000000      50.000000   \n",
      "std                              NaN       2.872283      14.430878   \n",
      "\n",
      "               sales  \n",
      "count  913000.000000  \n",
      "mean       52.246664  \n",
      "min         0.000000  \n",
      "25%        30.000000  \n",
      "50%        47.000000  \n",
      "75%        70.000000  \n",
      "max       173.000000  \n",
      "std        28.784893  \n"
     ]
    }
   ],
   "source": [
    "# Ver resultado de quitar outliers\n",
    "print(df.describe())\n",
    "\n",
    "# Ordenar por fecha\n",
    "df = df.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e34753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de datos\n",
    "# Codificar store e item\n",
    "store_enc = LabelEncoder()\n",
    "item_enc = LabelEncoder()\n",
    "df[\"store_id\"] = store_enc.fit_transform(df[\"store\"])\n",
    "df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "\n",
    "n_stores = df[\"store_id\"].nunique()\n",
    "n_items = df[\"item_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5ce4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos ventas por serie\n",
    "scalers = {}  # un scaler por serie\n",
    "for s in range(n_stores):\n",
    "    for i in range(n_items):\n",
    "        mask = (df[\"store_id\"]==s) & (df[\"item_id\"]==i)\n",
    "        scaler = MinMaxScaler()\n",
    "        df.loc[mask, \"sales_scaled\"] = scaler.fit_transform(df.loc[mask, [\"sales\"]])\n",
    "        scalers[(s,i)] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e83f50",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f139b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango de fechas: 2013-01-01 00:00:00 a 2017-12-31 00:00:00\n",
      "Total de días: 1826\n"
     ]
    }
   ],
   "source": [
    "# Obtener fechas únicas ordenadas\n",
    "fechas_unicas = sorted(df['date'].unique())\n",
    "print(f\"Rango de fechas: {fechas_unicas[0]} a {fechas_unicas[-1]}\")\n",
    "print(f\"Total de días: {len(fechas_unicas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7698662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "División temporal:\n",
      "Train: 2013-01-01 00:00:00 a 2017-07-04 00:00:00\n",
      "Validación: 2017-07-05 00:00:00 a 2017-10-02 00:00:00\n",
      "Test: 2017-10-03 00:00:00 a 2017-12-31 00:00:00\n",
      "\n",
      "Tamaños de conjuntos:\n",
      "Train: 823000 registros\n",
      "Validación: 45000 registros\n",
      "Test: 45000 registros\n"
     ]
    }
   ],
   "source": [
    "# Definir puntos de corte\n",
    "test_days = 90  # 3 meses\n",
    "val_days = 90   # 3 meses\n",
    "\n",
    "fecha_test_inicio = fechas_unicas[-test_days]\n",
    "fecha_val_inicio = fechas_unicas[-(test_days + val_days)]\n",
    "\n",
    "print(f\"\\nDivisión temporal:\")\n",
    "print(f\"Train: {fechas_unicas[0]} a {fechas_unicas[-(test_days + val_days + 1)]}\")\n",
    "print(f\"Validación: {fecha_val_inicio} a {fechas_unicas[-(test_days + 1)]}\")\n",
    "print(f\"Test: {fecha_test_inicio} a {fechas_unicas[-1]}\")\n",
    "\n",
    "# Crear conjuntos\n",
    "df_train = df[df['date'] < fecha_val_inicio].copy()\n",
    "df_val = df[(df['date'] >= fecha_val_inicio) & (df['date'] < fecha_test_inicio)].copy()\n",
    "df_test = df[df['date'] >= fecha_test_inicio].copy()\n",
    "\n",
    "print(f\"\\nTamaños de conjuntos:\")\n",
    "print(f\"Train: {len(df_train)} registros\")\n",
    "print(f\"Validación: {len(df_val)} registros\")\n",
    "print(f\"Test: {len(df_test)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c16f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generando secuencias...\n",
      "\n",
      "Forma de los conjuntos generados:\n",
      "X_train: (733500, 90), y_train: (733500, 90)\n",
      "X_val: (45000, 90), y_val: (45000, 90)\n",
      "X_test: (45000, 90), y_test: (45000, 90)\n",
      "\n",
      "Preparación de datos completada ✓\n"
     ]
    }
   ],
   "source": [
    "# Generación de secuencias\n",
    "# Ventana histórica de 90 días para predecir los próximos 90 días (3 meses)\n",
    "\n",
    "lookback = 90  # días históricos para usar\n",
    "forecast = 90  # días a predecir (3 meses)\n",
    "\n",
    "def create_sequences(data, store_id, item_id, lookback, forecast):\n",
    "    \"\"\"\n",
    "    Crea secuencias de entrada (X) y salida (y) para una serie temporal.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame completo\n",
    "        store_id: ID de la tienda\n",
    "        item_id: ID del item\n",
    "        lookback: días históricos a usar\n",
    "        forecast: días a predecir\n",
    "    \n",
    "    Returns:\n",
    "        X: array con secuencias de entrada (n_samples, lookback)\n",
    "        y: array con secuencias de salida (n_samples, forecast)\n",
    "        dates: fechas correspondientes a cada secuencia\n",
    "    \"\"\"\n",
    "    # Filtrar datos para esta combinación store-item\n",
    "    mask = (data['store_id'] == store_id) & (data['item_id'] == item_id)\n",
    "    serie = data[mask].sort_values('date')\n",
    "    \n",
    "    # Extraer ventas escaladas\n",
    "    values = serie['sales_scaled'].values\n",
    "    dates = serie['date'].values\n",
    "    \n",
    "    X, y, seq_dates = [], [], []\n",
    "    \n",
    "    # Crear ventanas deslizantes\n",
    "    for i in range(len(values) - lookback - forecast + 1):\n",
    "        X.append(values[i:i + lookback])\n",
    "        y.append(values[i + lookback:i + lookback + forecast])\n",
    "        seq_dates.append(dates[i + lookback - 1])  # fecha del último dato de entrada\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(seq_dates)\n",
    "\n",
    "\n",
    "# Generar secuencias para todos los store-item\n",
    "X_train_list, y_train_list = [], []\n",
    "X_val_list, y_val_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "print(\"\\nGenerando secuencias...\")\n",
    "for store_id in range(n_stores):\n",
    "    for item_id in range(n_items):\n",
    "        # Secuencias de entrenamiento\n",
    "        X_tr, y_tr, _ = create_sequences(df_train, store_id, item_id, lookback, forecast)\n",
    "        if len(X_tr) > 0:\n",
    "            X_train_list.append(X_tr)\n",
    "            y_train_list.append(y_tr)\n",
    "        \n",
    "        # Secuencias de validación (usar train + val para crear ventanas)\n",
    "        df_train_val = pd.concat([df_train, df_val])\n",
    "        X_v, y_v, dates_v = create_sequences(df_train_val, store_id, item_id, lookback, forecast)\n",
    "        # Filtrar solo secuencias que predicen en periodo de validación\n",
    "        mask_val = dates_v >= fecha_val_inicio - pd.Timedelta(days=lookback)\n",
    "        if mask_val.sum() > 0:\n",
    "            X_val_list.append(X_v[mask_val])\n",
    "            y_val_list.append(y_v[mask_val])\n",
    "        \n",
    "        # Secuencias de test (usar train + val + test para crear ventanas)\n",
    "        df_full = pd.concat([df_train, df_val, df_test])\n",
    "        X_t, y_t, dates_t = create_sequences(df_full, store_id, item_id, lookback, forecast)\n",
    "        # Filtrar solo secuencias que predicen en periodo de test\n",
    "        mask_test = dates_t >= fecha_test_inicio - pd.Timedelta(days=lookback)\n",
    "        if mask_test.sum() > 0:\n",
    "            X_test_list.append(X_t[mask_test])\n",
    "            y_test_list.append(y_t[mask_test])\n",
    "\n",
    "# Concatenar todas las secuencias\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "\n",
    "X_val = np.concatenate(X_val_list, axis=0)\n",
    "y_val = np.concatenate(y_val_list, axis=0)\n",
    "\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "print(f\"\\nForma de los conjuntos generados:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nPreparación de datos completada ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbdfe4",
   "metadata": {},
   "source": [
    "#### Descripción de las secuencias generadas.\n",
    "Hemos creado un conjunto de datos con arquitectura secuencia a secuencia (seq2seq), donde cada muestra contiene:\n",
    "* Entrada (X): 90 días consecutivos de ventas históricas (normalizadas)\n",
    "* Salida (y): Los siguientes 90 días de ventas que queremos predecir\n",
    "\n",
    "Usamos una ventana deslizante que se mueve día a día. Esto significa que de una serie temporal larga se generan múltiples ejemplos de entrenamiento:\n",
    "* Secuencia 1: días 1-90 → predice días 91-180\n",
    "* Secuencia 2: días 2-91 → predice días 92-181\n",
    "* Secuencia 3: días 3-92 → predice días 93-182\n",
    "* Y así sucesivamente\n",
    "\n",
    "Al usar esta estructura, No se predice un solo valor sino 90 días completos, lo cual es más útil para planificación empresarial.\n",
    "- 90 días de historia capturan múltiples ciclos semanales y mensuales\n",
    "- Al normalizar por cada combinación store-item, el modelo aprende patrones relativos, no valores absolutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d66b1",
   "metadata": {},
   "source": [
    "#### ¿Para qué modelo es útil esta estructura?\n",
    "1. LSTM (Long Short-Term Memory)\n",
    "- Diseñado específicamente para secuencias temporales, permite capturar patrones de largo plazo como estacionalidad, puede \"recordar\" tendencias de semanas o meses atrás. Ideal para 90 días de entrada → 90 días de salida\n",
    "2. Transformer con Attention\n",
    "- Puede identificar qué días pasados son más relevantes para predecir días futuros. Captura relaciones complejas (ej: \"las ventas del lunes dependen del lunes anterior\")\n",
    "3. Seq2Seq con Encoder-Decoder\n",
    "- Es la arquitectura clásica para este tipo de problemas, pues es muy intuitivo conceptualmente\n",
    "- Encoder: comprime los 90 días de entrada en una representación\n",
    "- Decoder: genera los 90 días de predicción paso a paso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
